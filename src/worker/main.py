#!/usr/bin/env python3
"""
Bloco 4: Worker de InferÃªncia em Tempo Real
Consome mensagens do Kafka, processa com Autoencoder (GPU) e detecta anomalias.
"""

import json
import time
import signal
import sys
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
import structlog

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from confluent_kafka import Consumer, KafkaError, KafkaException
import joblib
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# ConfiguraÃ§Ã£o de paths
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.utils.config import settings, get_kafka_consumer_config

# ============================================================================
# CONFIGURAÃ‡ÃƒO DE LOGGING
# ============================================================================
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer(),
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger(__name__)

# ============================================================================
# MÃ‰TRICAS PROMETHEUS
# ============================================================================
MESSAGES_PROCESSED = Counter(
    'worker_messages_processed_total',
    'Total messages processed by worker',
    ['status']
)

MESSAGES_RECEIVED = Counter(
    'worker_messages_received_total',
    'Total messages received from Kafka'
)

INFERENCE_LATENCY = Histogram(
    'worker_inference_latency_seconds',
    'Inference latency in seconds',
    ['model']
)

ANOMALIES_DETECTED = Counter(
    'worker_anomalies_detected_total',
    'Total anomalies detected',
    ['model']
)

GPU_MEMORY_USAGE = Gauge(
    'worker_gpu_memory_usage_bytes',
    'GPU memory usage in bytes'
)

PROCESSING_ERRORS = Counter(
    'worker_processing_errors_total',
    'Total processing errors',
    ['error_type']
)

# ============================================================================
# DEFINIÃ‡ÃƒO DO AUTOENCODER (mesma arquitetura do treinamento)
# ============================================================================
class Autoencoder(nn.Module):
    def __init__(self, input_dim: int = 5, latent_dim: int = 2):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, latent_dim)
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 8),
            nn.ReLU(),
            nn.Linear(8, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
            nn.Linear(32, input_dim)
        )
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed, latent

# ============================================================================
# CARREGAMENTO DE MODELOS E ARTEFATOS
# ============================================================================
def load_artifacts() -> Tuple[Any, Any, Dict, nn.Module, torch.device]:
    """
    Carrega todos os artefatos necessÃ¡rios para inferÃªncia.
    Returns: (scaler, iforest, thresholds, model, device)
    """
    logger.info("Loading artifacts...")
    
    # Definir dispositivo (GPU se disponÃ­vel)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # Caminhos dos artefatos
    artifacts_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "artifacts")
    
    # 1. Carregar scaler
    scaler_path = os.path.join(artifacts_dir, "scaler.pkl")
    scaler = joblib.load(scaler_path)
    logger.info(f"âœ… Scaler loaded from {scaler_path}")
    logger.info(f"   Features: {scaler.n_features_in_}")
    
    # 2. Carregar thresholds
    thresholds_path = os.path.join(artifacts_dir, "thresholds.json")
    with open(thresholds_path, 'r') as f:
        thresholds = json.load(f)
    logger.info(f"âœ… Thresholds loaded from {thresholds_path}")
    logger.info(f"   Autoencoder threshold: {thresholds['autoencoder_mse_threshold']:.6f}")
    
    # 3. Carregar Isolation Forest (opcional)
    iforest_path = os.path.join(artifacts_dir, "iforest.pkl")
    iforest = None
    if os.path.exists(iforest_path):
        iforest = joblib.load(iforest_path)
        logger.info(f"âœ… Isolation Forest loaded from {iforest_path}")
    else:
        logger.warning(f"Isolation Forest not found at {iforest_path}, using Autoencoder only")
    
    # 4. Carregar modelo PyTorch
    model_path = os.path.join(artifacts_dir, "autoencoder.pt")
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # Instanciar modelo
    model = Autoencoder(
        input_dim=checkpoint['input_dim'],
        latent_dim=checkpoint['latent_dim']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()  # Modo de avaliaÃ§Ã£o
    logger.info(f"âœ… Autoencoder loaded from {model_path}")
    logger.info(f"   Input dim: {checkpoint['input_dim']}")
    logger.info(f"   Latent dim: {checkpoint['latent_dim']}")
    logger.info(f"   Model moved to: {device}")
    
    # Verificar uso de GPU
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.memory_allocated() / 1e6
        logger.info(f"   GPU memory allocated: {gpu_mem:.2f} MB")
    
    return scaler, iforest, thresholds, model, device

# ============================================================================
# ENGENHARIA DE FEATURES ONLINE
# ============================================================================
def create_features_online(raw_data: Dict[str, Any]) -> Optional[np.ndarray]:
    """
    Converte dados brutos em features numÃ©ricas.
    Retorna None se houver erro nos dados.
    """
    try:
        # Extrair dados
        passenger_count = float(raw_data.get('passenger_count', 1))
        trip_distance = float(raw_data.get('trip_distance', 0))
        fare_amount = float(raw_data.get('fare_amount', 0))
        
        # Converter timestamps
        pickup_str = raw_data.get('pickup_datetime', '')
        dropoff_str = raw_data.get('dropoff_datetime', '')
        
        if not pickup_str or not dropoff_str:
            logger.warning("Missing datetime fields", raw_data=raw_data)
            return None
        
        # Converter para datetime
        pickup_dt = pd.to_datetime(pickup_str)
        dropoff_dt = pd.to_datetime(dropoff_str)
        
        # Calcular duraÃ§Ã£o em minutos
        trip_duration_min = (dropoff_dt - pickup_dt).total_seconds() / 60.0
        
        # Validar dados bÃ¡sicos
        if (passenger_count <= 0 or 
            trip_distance <= 0 or 
            fare_amount < 0 or 
            trip_duration_min <= 0):
            logger.warning("Invalid data values", 
                          passenger_count=passenger_count,
                          trip_distance=trip_distance,
                          fare_amount=fare_amount,
                          trip_duration_min=trip_duration_min)
            return None
        
        # Calcular fare_per_minute (com proteÃ§Ã£o contra divisÃ£o por zero)
        if trip_duration_min > 0:
            fare_per_minute = fare_amount / trip_duration_min
        else:
            logger.warning("Zero or negative trip duration", trip_duration_min=trip_duration_min)
            return None
        
        # Criar vetor de features
        features = np.array([
            passenger_count,
            trip_distance,
            fare_amount,
            trip_duration_min,
            fare_per_minute
        ]).reshape(1, -1)  # Shape: (1, 5)
        
        # Verificar NaNs ou infinitos
        if np.any(np.isnan(features)) or np.any(np.isinf(features)):
            logger.warning("NaN or Inf in features", features=features)
            return None
        
        return features
        
    except Exception as e:
        logger.error("Error creating features", error=str(e), raw_data=raw_data)
        PROCESSING_ERRORS.labels(error_type='feature_engineering').inc()
        return None

# ============================================================================
# INFERÃŠNCIA ENSEMBLE
# ============================================================================
def predict(
    raw_data: Dict[str, Any],
    scaler: Any,
    model: nn.Module,
    iforest: Any,
    thresholds: Dict[str, float],
    device: torch.device
) -> Tuple[bool, Dict[str, float], float]:
    """
    Executa inferÃªncia ensemble usando Autoencoder (GPU) e Isolation Forest (CPU).
    
    Returns:
        is_anomaly: bool
        scores: dict com scores de cada modelo
        inference_time: tempo total de inferÃªncia em segundos
    """
    start_time = time.time()
    
    # 1. Engenharia de features online
    features = create_features_online(raw_data)
    if features is None:
        return False, {'error': 'invalid_features'}, 0.0
    
    # 2. Aplicar scaling
    try:
        features_scaled = scaler.transform(features)
    except Exception as e:
        logger.error("Error in scaling", error=str(e))
        PROCESSING_ERRORS.labels(error_type='scaling').inc()
        return False, {'error': 'scaling_error'}, 0.0
    
    scores = {}
    
    # 3. InferÃªncia Isolation Forest (CPU)
    iso_anomaly_score = None
    if iforest is not None:
        try:
            # Isolation Forest espera dados nÃ£o escalados
            features_original = scaler.inverse_transform(features_scaled)
            iso_score = iforest.decision_function(features_original)[0]
            iso_anomaly = iso_score < 0  # Isolation Forest retorna -1 para anomalias
            
            scores['isolation_forest_score'] = float(iso_score)
            scores['isolation_forest_anomaly'] = bool(iso_anomaly)
            
        except Exception as e:
            logger.error("Isolation Forest inference error", error=str(e))
            PROCESSING_ERRORS.labels(error_type='isolation_forest').inc()
    
    # 4. InferÃªncia Autoencoder (GPU)
    try:
        # Converter para tensor PyTorch
        features_tensor = torch.FloatTensor(features_scaled).to(device)
        
        # InferÃªncia (sem calcular gradientes)
        with torch.no_grad():
            reconstructed, latent = model(features_tensor)
            
            # Calcular erro de reconstruÃ§Ã£o (MSE)
            reconstruction_error = torch.mean((reconstructed - features_tensor) ** 2).item()
            
            scores['autoencoder_mse'] = float(reconstruction_error)
            scores['autoencoder_latent'] = latent.cpu().numpy().tolist()
            
            # Verificar se Ã© anomalia baseado no threshold
            ae_threshold = thresholds['autoencoder_mse_threshold']
            ae_anomaly = reconstruction_error > ae_threshold
            scores['autoencoder_anomaly'] = bool(ae_anomaly)
    
    except Exception as e:
        logger.error("Autoencoder inference error", error=str(e))
        PROCESSING_ERRORS.labels(error_type='autoencoder').inc()
        return False, {'error': 'autoencoder_error'}, 0.0
    
    # 5. DecisÃ£o ensemble
    # Priorizar Autoencoder para este MVP
    is_anomaly = ae_anomaly
    
    # Se Isolation Forest tambÃ©m estÃ¡ disponÃ­vel, podemos combinar
    if iso_anomaly_score is not None:
        # LÃ³gica simples de combinaÃ§Ã£o: qualquer um detectar anomalia
        is_anomaly = ae_anomaly or iso_anomaly
    
    inference_time = time.time() - start_time
    INFERENCE_LATENCY.labels(model='ensemble').observe(inference_time)
    
    return is_anomaly, scores, inference_time

# ============================================================================
# CONSUMER KAFKA
# ============================================================================
class FraudDetectionWorker:
    def __init__(self):
        self.running = True
        self.scaler = None
        self.iforest = None
        self.thresholds = None
        self.model = None
        self.device = None
        
        # Configurar handlers de sinal
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
    
    def signal_handler(self, signum, frame):
        """Handler para sinais de shutdown."""
        logger.info(f"Received signal {signum}, shutting down...")
        self.running = False
    
    def initialize(self):
        """Inicializa o worker carregando artefatos e configurando Kafka."""
        logger.info("Initializing Fraud Detection Worker...")
        
        # 1. Carregar artefatos
        self.scaler, self.iforest, self.thresholds, self.model, self.device = load_artifacts()
        
        # 2. Configurar consumer Kafka
        consumer_config = get_kafka_consumer_config()
        self.consumer = Consumer(consumer_config)
        
        # 3. Subscrever tÃ³picos
        topics = [settings.KAFKA_TOPIC_TRANSACTIONS_RAW]
        self.consumer.subscribe(topics)
        logger.info(f"Subscribed to topics: {topics}")
        
        # 4. Iniciar servidor de mÃ©tricas
        metrics_port = 8001
        start_http_server(metrics_port)
        logger.info(f"Prometheus metrics server started on port {metrics_port}")
        
        logger.info("âœ… Worker initialization complete")
        logger.info(f"   Kafka: {settings.KAFKA_BOOTSTRAP_SERVERS}")
        logger.info(f"   Topics: {topics}")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   Autoencoder threshold: {self.thresholds['autoencoder_mse_threshold']:.6f}")
    
    def process_message(self, message) -> bool:
        """Processa uma mensagem Kafka."""
        MESSAGES_RECEIVED.inc()
        
        try:
            # Desserializar JSON
            raw_data = json.loads(message.value().decode('utf-8'))
            
            transaction_id = raw_data.get('transaction_id', 'unknown')
            request_id = raw_data.get('request_id', 'unknown')
            
            logger.debug("Processing message", 
                        transaction_id=transaction_id,
                        request_id=request_id)
            
            # Executar inferÃªncia
            is_anomaly, scores, inference_time = predict(
                raw_data, 
                self.scaler, 
                self.model, 
                self.iforest, 
                self.thresholds, 
                self.device
            )
            
            # Log baseado no resultado
            if is_anomaly:
                ANOMALIES_DETECTED.labels(model='autoencoder').inc()
                MESSAGES_PROCESSED.labels(status='anomaly').inc()
                
                logger.warning("ðŸš¨ ANOMALY DETECTED",
                              transaction_id=transaction_id,
                              request_id=request_id,
                              scores=scores,
                              inference_time=inference_time,
                              raw_data={k: v for k, v in raw_data.items() if k != 'received_at'})
            else:
                MESSAGES_PROCESSED.labels(status='normal').inc()
                
                logger.info("âœ… Transaction processed",
                           transaction_id=transaction_id,
                           request_id=request_id,
                           inference_time=inference_time,
                           autoencoder_mse=scores.get('autoencoder_mse'),
                           is_normal=True)
            
            return True
            
        except json.JSONDecodeError as e:
            logger.error("JSON decode error", error=str(e), message=message.value())
            PROCESSING_ERRORS.labels(error_type='json_decode').inc()
            MESSAGES_PROCESSED.labels(status='error').inc()
            return False
            
        except Exception as e:
            logger.error("Unexpected error processing message", 
                        error=str(e), 
                        error_type=type(e).__name__)
            PROCESSING_ERRORS.labels(error_type='unexpected').inc()
            MESSAGES_PROCESSED.labels(status='error').inc()
            return False
    
    def run(self):
        """Loop principal de consumo Kafka."""
        logger.info("Starting Kafka consumer loop...")
        
        poll_timeout = 1.0  # segundos
        
        while self.running:
            try:
                # Poll para nova mensagem
                msg = self.consumer.poll(poll_timeout)
                
                if msg is None:
                    # Timeout, continuar loop
                    continue
                
                if msg.error():
                    # Erro do Kafka
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        # Fim da partiÃ§Ã£o, normal
                        logger.debug("Reached end of partition", topic=msg.topic(), partition=msg.partition())
                    else:
                        logger.error("Kafka error", error=msg.error())
                    continue
                
                # Processar mensagem
                success = self.process_message(msg)
                
                # Commit manual do offset (se configurado)
                # self.consumer.commit(asynchronous=False)
                
            except KeyboardInterrupt:
                logger.info("Keyboard interrupt received")
                self.running = False
                break
                
            except Exception as e:
                logger.error("Error in consumer loop", error=str(e))
                time.sleep(1)  # Pequena pausa antes de continuar
        
        # Cleanup
        self.shutdown()
    
    def shutdown(self):
        """Cleanup resources."""
        logger.info("Shutting down worker...")
        
        try:
            # Fechar consumer Kafka
            if hasattr(self, 'consumer'):
                self.consumer.close()
                logger.info("Kafka consumer closed")
        except Exception as e:
            logger.error("Error closing Kafka consumer", error=str(e))
        
        # Limpar memÃ³ria GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("GPU cache cleared")
        
        logger.info("Worker shutdown complete")

# ============================================================================
# MAIN
# ============================================================================
def main():
    """FunÃ§Ã£o principal."""
    print("="*70)
    print("FRAUD DETECTION WORKER - BLOCO 4")
    print("="*70)
    
    worker = FraudDetectionWorker()
    
    try:
        worker.initialize()
        worker.run()
    except Exception as e:
        logger.critical("Fatal error in worker", error=str(e))
        worker.shutdown()
        sys.exit(1)

if __name__ == "__main__":
    main()
